{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "pdf_path = r\"C:\\email_extraction\\data.pdf\"\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = extract_text(file)\n",
    "        \n",
    "    return reader\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path= r\"C:\\email_extraction\\data.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rom:  CRM \\n\\nTeam <khushi2003p@gmail.com> \\n\\nto:  khushiq.86@gmail.com \\n\\nbcc:  khushi2003p@gmail.com \\n\\ndate: \\n\\nJun 12, 2024, 11:41\\u202fPM \\n\\nsubject:  Missed You! \\n\\nmailed-\\nby: \\n\\ngmail.com \\n\\n    Hey there, sakshi! \\n\\n    Guess what? The stars have aligned, and we've noticed that it's been a little while since you last \\nshopped with us. We've really missed your sparkling presence in our online aisles!  \\n\\n    But fear not, because your favorite Insight Sync team is here to welcome you back with open \\narms and a galaxy of fantastic deals!  \\n\\n    Life gets busy, we totally get it. But we're thrilled to see you back in orbit with us. To celebrate \\nyour return, we've conjured up something special just for you: \\n\\n     A cosmic 20% discount on your next purchase!  \\n\\n    Use code: COSMIC20 at checkout to blast off into savings! This discount is our way of saying \\nthank you for being an interstellar part of the [Your Store Name] family. \\n\\n    Whether you're searching for stellar new styles, cosmic home decor, or out-of-this-world \\ngadgets, we've got everything you need to make your universe a little brighter.  \\n\\n    So what are you waiting for? Dive back into our celestial collection and discover galaxies of \\ngoodness waiting just for you! \\n\\n    And remember, if you ever need a guiding star or have any questions, our stellar customer \\nsupport team is here to assist you every step of the way. \\n\\n    Welcome back to the cosmos, sakshi! We can't wait to embark on this cosmic journey with you \\nonce again. \\n\\n-- \\nThis email has been checked for viruses by Avast antivirus software. \\nwww.avast.com \\n\\nfrom:  CRM \\n\\nTeam <khushi2003p@gmail.com> \\n\\nto:  khushi11.03p@gmail.com \\n\\nbcc:  khushi2003p@gmail.com \\n\\n1 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cdate: \\n\\nJun 25, 2024, 11:19\\u202fAM \\n\\nsubject:  Missed You! \\n\\nmailed-\\nby: \\n\\nsigned-\\nby: \\n\\nsecurity: \\n\\ngmail.com \\n\\ngmail.com \\n\\n Standard encryption (TLS) Learn \\nmore \\n\\n    Hey there, Khushi! \\n\\n    Thankyou for connecting with us . This is a great chance for us. Please help us out . \\n\\n-- \\nThis email has been checked for viruses by Avast antivirus software. \\nwww.avast.com \\nOne attachment • Scanned by Gmail \\n\\n2 \\n\\n \\n \\n \\n \\n \\n \\n  \\n\\x0c\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 17083, 1024, 13675, 2213, 2136, 1026, 1047, 9825, 4048, 28332, 2509, 2361, 1030, 20917, 4014, 1012, 4012, 1028, 2000, 1024, 1047, 9825, 4048, 4160, 1012, 6564, 1030, 20917, 4014, 1012, 4012, 4647, 2278, 1024, 1047, 9825, 4048, 28332, 2509, 2361, 1030, 20917, 4014, 1012, 4012, 3058, 1024, 12022, 2260, 1010, 16798, 2549, 1010, 2340, 1024, 4601, 7610, 3395, 1024, 4771, 2017, 999, 5653, 2098, 1011, 2011, 1024, 20917, 4014, 1012, 4012, 4931, 2045, 1010, 7842, 27488, 999, 3984, 2054, 1029, 1996, 3340, 2031, 13115, 1010, 1998, 2057, 1005, 2310, 4384, 2008, 2009, 1005, 1055, 2042, 1037, 2210, 2096, 2144, 2017, 2197, 4497, 5669, 2007, 2149, 1012, 2057, 1005, 2310, 2428, 4771, 2115, 16619, 3739, 1999, 2256, 3784, 25442, 999, 2021, 3571, 2025, 1010, 2138, 2115, 5440, 12369, 26351, 2136, 2003, 2182, 2000, 6160, 2017, 2067, 2007, 2330, 2608, 1998, 1037, 9088, 1997, 10392, 9144, 999, 2166, 4152, 5697, 1010, 2057, 6135, 2131, 2009, 1012, 2021, 2057, 1005, 2128, 16082, 2000, 2156, 2017, 2067, 1999, 8753, 2007, 2149, 1012, 2000, 8439, 2115, 2709, 1010, 2057, 1005, 2310, 9530, 26949, 2039, 2242, 2569, 2074, 2005, 2017, 1024, 1037, 14448, 2322, 1003, 19575, 2006, 2115, 2279, 5309, 999, 2224, 3642, 1024, 14448, 11387, 2012, 4638, 5833, 2000, 8479, 2125, 2046, 10995, 999, 2023, 19575, 2003, 2256, 2126, 1997, 3038, 4067, 2017, 2005, 2108, 2019, 6970, 29028, 2112, 1997, 1996, 1031, 2115, 3573, 2171, 1033, 2155, 1012, 3251, 2017, 1005, 2128, 6575, 2005, 17227, 2047, 6782, 1010, 14448, 2188, 25545, 1010, 2030, 2041, 1011, 1997, 1011, 2023, 1011, 2088, 11721, 28682, 1010, 2057, 1005, 2310, 2288, 2673, 2017, 2342, 2000, 2191, 2115, 5304, 1037, 2210, 16176, 1012, 2061, 2054, 2024, 2017, 3403, 2005, 1029, 11529, 2067, 2046, 2256, 17617, 3074, 1998, 7523, 21706, 1997, 15003, 3403, 2074, 2005, 2017, 999, 1998, 3342, 1010, 2065, 2017, 2412, 2342, 1037, 14669, 2732, 2030, 2031, 2151, 3980, 1010, 2256, 17227, 8013, 2490, 2136, 2003, 2182, 2000, 6509, 2017, 2296, 3357, 1997, 1996, 2126, 1012, 6160, 2067, 2000, 1996, 21182, 1010, 7842, 27488, 999, 2057, 2064, 1005, 1056, 3524, 2000, 28866, 2006, 2023, 14448, 4990, 2007, 2017, 2320, 2153, 1012, 1011, 1011, 2023, 10373, 2038, 2042, 7039, 2005, 18191, 2011, 10927, 3367, 3424, 23350, 4007, 1012, 7479, 1012, 10927, 3367, 1012, 4012, 2013, 1024, 13675, 2213, 2136, 1026, 1047, 9825, 4048, 28332, 2509, 2361, 1030, 20917, 4014, 1012, 4012, 1028, 2000, 1024, 1047, 9825, 4048, 14526, 1012, 6021, 2361, 1030, 20917, 4014, 1012, 4012, 4647, 2278, 1024, 1047, 9825, 4048, 28332, 2509, 2361, 1030, 20917, 4014, 1012, 4012, 1015, 3058, 1024, 12022, 2423, 1010, 16798, 2549, 1010, 2340, 1024, 2539, 2572, 3395, 1024, 4771, 2017, 999, 5653, 2098, 1011, 2011, 1024, 2772, 1011, 2011, 1024, 3036, 1024, 20917, 4014, 1012, 4012, 20917, 4014, 1012, 4012, 3115, 21999, 1006, 1056, 4877, 1007, 4553, 2062, 4931, 2045, 1010, 1047, 9825, 4048, 999, 4067, 29337, 2005, 7176, 2007, 2149, 1012, 2023, 2003, 1037, 2307, 3382, 2005, 2149, 1012, 3531, 2393, 2149, 2041, 1012, 1011, 1011, 2023, 10373, 2038, 2042, 7039, 2005, 18191, 2011, 10927, 3367, 3424, 23350, 4007, 1012, 7479, 1012, 10927, 3367, 1012, 4012, 2028, 14449, 1528, 11728, 2011, 20917, 4014, 1016, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'rom',\n",
       " ':',\n",
       " 'cr',\n",
       " '##m',\n",
       " 'team',\n",
       " '<',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '##200',\n",
       " '##3',\n",
       " '##p',\n",
       " '@',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " '>',\n",
       " 'to',\n",
       " ':',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '##q',\n",
       " '.',\n",
       " '86',\n",
       " '@',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " 'bc',\n",
       " '##c',\n",
       " ':',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '##200',\n",
       " '##3',\n",
       " '##p',\n",
       " '@',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " 'date',\n",
       " ':',\n",
       " 'jun',\n",
       " '12',\n",
       " ',',\n",
       " '202',\n",
       " '##4',\n",
       " ',',\n",
       " '11',\n",
       " ':',\n",
       " '41',\n",
       " 'pm',\n",
       " 'subject',\n",
       " ':',\n",
       " 'missed',\n",
       " 'you',\n",
       " '!',\n",
       " 'mail',\n",
       " '##ed',\n",
       " '-',\n",
       " 'by',\n",
       " ':',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " 'hey',\n",
       " 'there',\n",
       " ',',\n",
       " 'sa',\n",
       " '##kshi',\n",
       " '!',\n",
       " 'guess',\n",
       " 'what',\n",
       " '?',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'have',\n",
       " 'aligned',\n",
       " ',',\n",
       " 'and',\n",
       " 'we',\n",
       " \"'\",\n",
       " 've',\n",
       " 'noticed',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'been',\n",
       " 'a',\n",
       " 'little',\n",
       " 'while',\n",
       " 'since',\n",
       " 'you',\n",
       " 'last',\n",
       " 'shop',\n",
       " '##ped',\n",
       " 'with',\n",
       " 'us',\n",
       " '.',\n",
       " 'we',\n",
       " \"'\",\n",
       " 've',\n",
       " 'really',\n",
       " 'missed',\n",
       " 'your',\n",
       " 'sparkling',\n",
       " 'presence',\n",
       " 'in',\n",
       " 'our',\n",
       " 'online',\n",
       " 'aisles',\n",
       " '!',\n",
       " 'but',\n",
       " 'fear',\n",
       " 'not',\n",
       " ',',\n",
       " 'because',\n",
       " 'your',\n",
       " 'favorite',\n",
       " 'insight',\n",
       " 'sync',\n",
       " 'team',\n",
       " 'is',\n",
       " 'here',\n",
       " 'to',\n",
       " 'welcome',\n",
       " 'you',\n",
       " 'back',\n",
       " 'with',\n",
       " 'open',\n",
       " 'arms',\n",
       " 'and',\n",
       " 'a',\n",
       " 'galaxy',\n",
       " 'of',\n",
       " 'fantastic',\n",
       " 'deals',\n",
       " '!',\n",
       " 'life',\n",
       " 'gets',\n",
       " 'busy',\n",
       " ',',\n",
       " 'we',\n",
       " 'totally',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " 'but',\n",
       " 'we',\n",
       " \"'\",\n",
       " 're',\n",
       " 'thrilled',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " 'back',\n",
       " 'in',\n",
       " 'orbit',\n",
       " 'with',\n",
       " 'us',\n",
       " '.',\n",
       " 'to',\n",
       " 'celebrate',\n",
       " 'your',\n",
       " 'return',\n",
       " ',',\n",
       " 'we',\n",
       " \"'\",\n",
       " 've',\n",
       " 'con',\n",
       " '##jured',\n",
       " 'up',\n",
       " 'something',\n",
       " 'special',\n",
       " 'just',\n",
       " 'for',\n",
       " 'you',\n",
       " ':',\n",
       " 'a',\n",
       " 'cosmic',\n",
       " '20',\n",
       " '%',\n",
       " 'discount',\n",
       " 'on',\n",
       " 'your',\n",
       " 'next',\n",
       " 'purchase',\n",
       " '!',\n",
       " 'use',\n",
       " 'code',\n",
       " ':',\n",
       " 'cosmic',\n",
       " '##20',\n",
       " 'at',\n",
       " 'check',\n",
       " '##out',\n",
       " 'to',\n",
       " 'blast',\n",
       " 'off',\n",
       " 'into',\n",
       " 'savings',\n",
       " '!',\n",
       " 'this',\n",
       " 'discount',\n",
       " 'is',\n",
       " 'our',\n",
       " 'way',\n",
       " 'of',\n",
       " 'saying',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'being',\n",
       " 'an',\n",
       " 'inter',\n",
       " '##stellar',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " '[',\n",
       " 'your',\n",
       " 'store',\n",
       " 'name',\n",
       " ']',\n",
       " 'family',\n",
       " '.',\n",
       " 'whether',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'searching',\n",
       " 'for',\n",
       " 'stellar',\n",
       " 'new',\n",
       " 'styles',\n",
       " ',',\n",
       " 'cosmic',\n",
       " 'home',\n",
       " 'decor',\n",
       " ',',\n",
       " 'or',\n",
       " 'out',\n",
       " '-',\n",
       " 'of',\n",
       " '-',\n",
       " 'this',\n",
       " '-',\n",
       " 'world',\n",
       " 'ga',\n",
       " '##dgets',\n",
       " ',',\n",
       " 'we',\n",
       " \"'\",\n",
       " 've',\n",
       " 'got',\n",
       " 'everything',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'make',\n",
       " 'your',\n",
       " 'universe',\n",
       " 'a',\n",
       " 'little',\n",
       " 'brighter',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'you',\n",
       " 'waiting',\n",
       " 'for',\n",
       " '?',\n",
       " 'dive',\n",
       " 'back',\n",
       " 'into',\n",
       " 'our',\n",
       " 'celestial',\n",
       " 'collection',\n",
       " 'and',\n",
       " 'discover',\n",
       " 'galaxies',\n",
       " 'of',\n",
       " 'goodness',\n",
       " 'waiting',\n",
       " 'just',\n",
       " 'for',\n",
       " 'you',\n",
       " '!',\n",
       " 'and',\n",
       " 'remember',\n",
       " ',',\n",
       " 'if',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'need',\n",
       " 'a',\n",
       " 'guiding',\n",
       " 'star',\n",
       " 'or',\n",
       " 'have',\n",
       " 'any',\n",
       " 'questions',\n",
       " ',',\n",
       " 'our',\n",
       " 'stellar',\n",
       " 'customer',\n",
       " 'support',\n",
       " 'team',\n",
       " 'is',\n",
       " 'here',\n",
       " 'to',\n",
       " 'assist',\n",
       " 'you',\n",
       " 'every',\n",
       " 'step',\n",
       " 'of',\n",
       " 'the',\n",
       " 'way',\n",
       " '.',\n",
       " 'welcome',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'cosmos',\n",
       " ',',\n",
       " 'sa',\n",
       " '##kshi',\n",
       " '!',\n",
       " 'we',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'embark',\n",
       " 'on',\n",
       " 'this',\n",
       " 'cosmic',\n",
       " 'journey',\n",
       " 'with',\n",
       " 'you',\n",
       " 'once',\n",
       " 'again',\n",
       " '.',\n",
       " '-',\n",
       " '-',\n",
       " 'this',\n",
       " 'email',\n",
       " 'has',\n",
       " 'been',\n",
       " 'checked',\n",
       " 'for',\n",
       " 'viruses',\n",
       " 'by',\n",
       " 'ava',\n",
       " '##st',\n",
       " 'anti',\n",
       " '##virus',\n",
       " 'software',\n",
       " '.',\n",
       " 'www',\n",
       " '.',\n",
       " 'ava',\n",
       " '##st',\n",
       " '.',\n",
       " 'com',\n",
       " 'from',\n",
       " ':',\n",
       " 'cr',\n",
       " '##m',\n",
       " 'team',\n",
       " '<',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '##200',\n",
       " '##3',\n",
       " '##p',\n",
       " '@',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " '>',\n",
       " 'to',\n",
       " ':',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '##11',\n",
       " '.',\n",
       " '03',\n",
       " '##p',\n",
       " '@',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " 'bc',\n",
       " '##c',\n",
       " ':',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '##200',\n",
       " '##3',\n",
       " '##p',\n",
       " '@',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " '1',\n",
       " 'date',\n",
       " ':',\n",
       " 'jun',\n",
       " '25',\n",
       " ',',\n",
       " '202',\n",
       " '##4',\n",
       " ',',\n",
       " '11',\n",
       " ':',\n",
       " '19',\n",
       " 'am',\n",
       " 'subject',\n",
       " ':',\n",
       " 'missed',\n",
       " 'you',\n",
       " '!',\n",
       " 'mail',\n",
       " '##ed',\n",
       " '-',\n",
       " 'by',\n",
       " ':',\n",
       " 'signed',\n",
       " '-',\n",
       " 'by',\n",
       " ':',\n",
       " 'security',\n",
       " ':',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " 'gma',\n",
       " '##il',\n",
       " '.',\n",
       " 'com',\n",
       " 'standard',\n",
       " 'encryption',\n",
       " '(',\n",
       " 't',\n",
       " '##ls',\n",
       " ')',\n",
       " 'learn',\n",
       " 'more',\n",
       " 'hey',\n",
       " 'there',\n",
       " ',',\n",
       " 'k',\n",
       " '##hus',\n",
       " '##hi',\n",
       " '!',\n",
       " 'thank',\n",
       " '##you',\n",
       " 'for',\n",
       " 'connecting',\n",
       " 'with',\n",
       " 'us',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'chance',\n",
       " 'for',\n",
       " 'us',\n",
       " '.',\n",
       " 'please',\n",
       " 'help',\n",
       " 'us',\n",
       " 'out',\n",
       " '.',\n",
       " '-',\n",
       " '-',\n",
       " 'this',\n",
       " 'email',\n",
       " 'has',\n",
       " 'been',\n",
       " 'checked',\n",
       " 'for',\n",
       " 'viruses',\n",
       " 'by',\n",
       " 'ava',\n",
       " '##st',\n",
       " 'anti',\n",
       " '##virus',\n",
       " 'software',\n",
       " '.',\n",
       " 'www',\n",
       " '.',\n",
       " 'ava',\n",
       " '##st',\n",
       " '.',\n",
       " 'com',\n",
       " 'one',\n",
       " 'attachment',\n",
       " '•',\n",
       " 'scanned',\n",
       " 'by',\n",
       " 'gma',\n",
       " '##il',\n",
       " '2',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"rom: CRM Team <khushi2003p@gmail.com> to: khushiq.86@gmail.com bcc: khushi2003p@gmail.com date: Jun 12, 2024, 11:41 PM subject: Missed You! mailed- by: gmail.com Hey there, sakshi! Guess what? stars aligned, we've noticed little since last shopped us. We've really missed sparkling presence online aisles! fear not, favorite Insight Sync team welcome back open arms galaxy fantastic deals! Life gets busy, totally get it. we're thrilled see back orbit us. celebrate return, we've conjured something special you: cosmic 20% discount next purchase! Use code: COSMIC20 checkout blast savings! discount way saying thank interstellar part [Your Store Name] family. Whether searching stellar new styles, cosmic home decor, out-of-this-world gadgets, we've got everything need make universe little brighter. waiting for? Dive back celestial collection discover galaxies goodness waiting you! remember, ever need guiding star questions, stellar customer support team assist every step way. Welcome back cosmos, sakshi! can't wait embark cosmic journey again. -- email checked viruses Avast antivirus software. www.avast.com from: CRM Team <khushi2003p@gmail.com> to: khushi11.03p@gmail.com bcc: khushi2003p@gmail.com 1 date: Jun 25, 2024, 11:19 subject: Missed You! mailed- by: signed- by: security: gmail.com gmail.com Standard encryption (TLS) Learn Hey there, Khushi! Thankyou connecting us . great chance us. Please help us . -- email checked viruses Avast antivirus software. www.avast.com One attachment • Scanned Gmail 2\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(text):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_text = ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "    return filtered_text\n",
    "remove_stop_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for token in tokens:\n",
    "    if \"#\" in list(token) or \"@\" in list(token) or \"[CLS]\" == token:\n",
    "        pass\n",
    "    else:\n",
    "        arr.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \" \"\n",
    "for element in arr:\n",
    "    str = str+ \" \" +element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  rom   cr team   k gma   com   to   k   86 gma   com bc   k gma   com date   jun 12   202   11   41 pm subject   missed you   mail   by   gma   com hey there   sa   guess what   the stars have aligned   and we   ve noticed that it   s been a little while since you last shop with us   we   ve really missed your sparkling presence in our online aisles   but fear not   because your favorite insight sync team is here to welcome you back with open arms and a galaxy of fantastic deals   life gets busy   we totally get it   but we   re thrilled to see you back in orbit with us   to celebrate your return   we   ve con up something special just for you   a cosmic 20   discount on your next purchase   use code   cosmic at check to blast off into savings   this discount is our way of saying thank you for being an inter part of the   your store name   family   whether you   re searching for stellar new styles   cosmic home decor   or out   of   this   world ga   we   ve got everything you need to make your universe a little brighter   so what are you waiting for   dive back into our celestial collection and discover galaxies of goodness waiting just for you   and remember   if you ever need a guiding star or have any questions   our stellar customer support team is here to assist you every step of the way   welcome back to the cosmos   sa   we can   t wait to embark on this cosmic journey with you once again       this email has been checked for viruses by ava anti software   www   ava   com from   cr team   k gma   com   to   k   03 gma   com bc   k gma   com 1 date   jun 25   202   11   19 am subject   missed you   mail   by   signed   by   security   gma   com gma   com standard encryption   t   learn more hey there   k   thank for connecting with us   this is a great chance for us   please help us out       this email has been checked for viruses by ava anti software   www   ava   com one attachment   scanned by gma 2  SEP '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m remove_special_characters(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msummarizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_message\n\u001b[1;32m---> 48\u001b[0m text_final , count \u001b[38;5;241m=\u001b[39m generate_message(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#print(text_final)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_text.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import faiss\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "#pdf_path = r\"C:\\Users\\HP\\Downloads\\data.pdf\"\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = extract_text(file)\n",
    "        \n",
    "    return reader\n",
    "\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path= r\"C:\\email_extraction\\data.pdf\")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "from transformers import DistilBertTokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "encoded_text = tokenizer(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "def remove_stop_words(text):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_text = ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "    return filtered_text\n",
    "remove_stop_words(text)\n",
    "arr = []\n",
    "for token in tokens:\n",
    "    if \"#\" in list(token) or \"@\" in list(token) or \"[CLS]\" == token:\n",
    "        pass\n",
    "    else:\n",
    "        arr.append(token)\n",
    "\n",
    "str = \" \"\n",
    "for element in arr:\n",
    "    str = str+ \" \" +element\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "\n",
    "remove_special_characters(str)\n",
    "\n",
    "from summarizer import generate_message\n",
    "text_final , count = generate_message(str)\n",
    "#print(text_final)\n",
    "\n",
    "output_file_path = \"output_text.txt\"\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text_final)\n",
    "\n",
    "def extraction_and_summarization(pdf_path):\n",
    "    extract_text_from_pdf(pdf_path)\n",
    "    model_ckpt = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "    distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "    encoded_text = tokenizer(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "    remove_stop_words(text)\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        if \"#\" in list(token) or \"@\" in list(token) or \"[CLS]\" == token:\n",
    "            pass\n",
    "        else:\n",
    "            arr.append(token)\n",
    "    str = \" \"\n",
    "    for element in arr:\n",
    "        str = str+ \" \" +element\n",
    "    \n",
    "    remove_special_characters(str)\n",
    "    text_final = generate_message(str)\n",
    "    print(text_final)\n",
    "    output_file_path = \"output_text.txt\"\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
